{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad7fe345-4f4e-4108-8df4-59140d21e013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Check if all files are uploaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d95da25e-b820-4312-97eb-7f7d311f276c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/FileStore/tables/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e72c769f-06da-4c89-8355-d6ff2ecf91a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Merge the files and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af42e58a-dc2c-4be6-aac5-cc57e3a2ec67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load all CSV files while letting Databricks handle headers automatically\n",
    "crime_df = spark.read.option(\"header\", \"true\").csv(\"/FileStore/tables/CrimeDataset_*.csv\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2123ae74-773a-4118-a21c-20412d12c61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show first few rows to verify correct loading\n",
    "crime_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72d97ad8-6422-4ce7-a6ec-db476f818491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check total number of records\n",
    "print(f\"Total records in dataset: {crime_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce47a53a-2548-4378-b000-c528485bd6b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To perform dataset cleaning, the first is to check for missing values of each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d06add32-1d9b-4a29-b701-9f21a034c4f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Count missing values per column\n",
    "missing_values = crime_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in crime_df.columns])\n",
    "\n",
    "# Show the result\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5c6325d-8936-4a4e-b1db-5d72ed00c2aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For better readability, I converted the number of missing values of each column into table, which can further be visualized for even better readability. In the table, I can easily identify the attributes(columns) that matters to my future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9325619d-3936-4339-b4b3-1dcc9f148657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "missing_values_pd = missing_values.toPandas()\n",
    "\n",
    "# Transpose the DataFrame to have columns as index\n",
    "missing_values_pd = missing_values_pd.T  # Flip rows and columns\n",
    "\n",
    "# Create a new column for proper labels\n",
    "missing_values_pd.reset_index(inplace=True)\n",
    "missing_values_pd.columns = [\"Column Name\", \"Missing Count\"]\n",
    "\n",
    "# Sort by the highest number of missing values\n",
    "missing_values_pd = missing_values_pd.sort_values(by=\"Missing Count\", ascending=False)\n",
    "\n",
    "# Display as a table\n",
    "display(missing_values_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1771dcb-b200-4d14-96bd-e4dbd61094ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I identify that \"Vict Sex\" is one of the column that I would be using. Thus, I first want to know all its values and then decide what to do with the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eca69f7-668a-42ec-80bb-36cee9c1df46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crime_df.groupBy(\"Vict Sex\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8092338-a547-49ae-b1e1-06056c773dcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As the originally in the column there is already an \"X\", meaing \"sex unknown\", thus I decided to transform all sex other than \"M\" and \"F\", including \"null\", into \"X\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9982f772-0e49-45b3-8f9a-4c58c1678703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Standardize 'Vict Sex' column\n",
    "crime_df = crime_df.withColumn(\n",
    "    \"Vict Sex\",\n",
    "    when(crime_df[\"Vict Sex\"].isin(\"M\", \"F\", \"X\"), crime_df[\"Vict Sex\"])\n",
    "    .otherwise(\"X\")  # Replace invalid values with 'X' (Unknown)\n",
    ")\n",
    "\n",
    "crime_df.groupBy(\"Vict Sex\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faa5958a-1b5e-4476-95ea-1a7ffb56fdb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next I also identify \"Vict Descent\" as one of the attribute I want to work with, therefore I want to check the counts of each value and how the \"null\" value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83844ebb-a437-439e-80fa-9a01f0682ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crime_df.groupBy(\"Vict Descent\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98b8bb97-0825-4005-aa24-f20d8a3531a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Similar to \"Vict Sex\", \"Vict Descent\" already has a \"X\" value that represents \"unknown\". Thus I decided to transform all null values and other values not listed in the original dataset explanation into \"X\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74cd8a58-8c32-4739-8158-031ac7142bd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of valid descent codes\n",
    "valid_descents = [\"A\", \"B\", \"C\", \"D\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"O\", \"P\", \"S\", \"U\", \"V\", \"W\", \"X\", \"Z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f9d9bf-fd95-4b37-a32e-c05318240fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardize 'Vict Descent' column\n",
    "crime_df = crime_df.withColumn(\n",
    "    \"Vict Descent\",\n",
    "    when(crime_df[\"Vict Descent\"].isin(valid_descents), crime_df[\"Vict Descent\"])\n",
    "    .otherwise(\"X\")  # Replace invalid values with 'X' (Unknown)\n",
    ")\n",
    "crime_df.groupBy(\"Vict Descent\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0f1f208-9f07-4ad3-97d1-d87c7fcadcec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After performing data cleaning, I want to start the analysis. First is to see the overall top crimes occurred. I use plotly to create a bar chart that shows the Top 10 crimes occurred during the period of the dataset (2020-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88932a1d-f50a-487c-a862-bdfb9e504c53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Aggregate top 10 crimes\n",
    "top_crimes_df = crime_df.groupBy(\"Crm Cd Desc\").count().orderBy(\"count\", ascending=False).limit(10)\n",
    "\n",
    "# Convert to Pandas DataFrame for Plotly\n",
    "top_crimes_pd = top_crimes_df.toPandas()\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "fig = px.bar(\n",
    "    top_crimes_pd, \n",
    "    x=\"count\",  # Horizontal bars\n",
    "    y=\"Crm Cd Desc\",  # Crime Types on Y-axis\n",
    "    title=\"Top 10 Crimes Occurred Overall\",\n",
    "    labels={\"Crm Cd Desc\": \"Crime Type\", \"count\": \"Number of Occurrences\"},\n",
    "    color=\"Crm Cd Desc\",  # Assign different colors for each crime type\n",
    "    color_discrete_sequence=px.colors.qualitative.Set3,  # Color palette\n",
    "    orientation=\"h\"  # Makes the chart horizontal\n",
    ")\n",
    "\n",
    "# Remove legend\n",
    "fig.update_layout(showlegend=False)\n",
    "\n",
    "# Improve layout\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Number of Occurrences\",\n",
    "    yaxis_title=\"Crime Type\"\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5afc043-9f96-4fa2-8b8b-bebf3b7299c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, I want to see the trend of the top 10 crimes of each year (2020, 2021, 2022, 2023 and 2024). So I extracted the \"year\" from \"DATE OCC\" column and created a new column name \"Year\" to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "516e7f4d-4245-4dae-a9ef-34d7387b0999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, to_date\n",
    "\n",
    "# Convert \"DATE OCC\" to a proper date format and extract the year again\n",
    "crime_df = crime_df.withColumn(\"Year\", year(to_date(crime_df[\"DATE OCC\"], \"MM/dd/yyyy hh:mm:ss a\")))\n",
    "\n",
    "# Check extracted years\n",
    "crime_df.select(\"DATE OCC\", \"Year\").distinct().show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34bb6a9a-63f9-4a2d-8de7-9e918fa2562d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "And then we make sure there no \"null\" value in the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a92f2777-7984-483f-84fd-896741966e7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crime_df = crime_df.filter(crime_df[\"Year\"].isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "012029b8-820d-4d84-9692-c1fb64464dc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, I aggregate top 10 crime trends over years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa0cde1-ee74-42bd-be51-d88a10b8ffb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, desc\n",
    "\n",
    "# Compute the top 10 crimes list\n",
    "top_crimes_df = (\n",
    "    crime_df.groupBy(\"Crm Cd Desc\")\n",
    "    .agg(count(\"*\").alias(\"count\"))\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "# Convert to list\n",
    "top_crimes_list = [row[\"Crm Cd Desc\"] for row in top_crimes_df.collect()]\n",
    "\n",
    "# Check if list is empty before proceeding\n",
    "if not top_crimes_list:\n",
    "    raise ValueError(\"Error: `top_crimes_list` is empty. Check if `crime_df` contains valid crime descriptions.\")\n",
    "\n",
    "# Filter dataset for only top 10 crimes\n",
    "crime_trend_df = crime_df.filter(col(\"Crm Cd Desc\").isin(top_crimes_list))\n",
    "\n",
    "# Group by Year and Crime Type\n",
    "crime_trend_df = crime_trend_df.groupBy(\"Year\", \"Crm Cd Desc\").count()\n",
    "\n",
    "# Convert to Pandas for Plotly\n",
    "crime_trend_pd = crime_trend_df.toPandas()\n",
    "\n",
    "# Ensure Year column is sorted correctly\n",
    "crime_trend_pd = crime_trend_pd.sort_values(by=[\"Year\", \"count\"], ascending=[True, False])\n",
    "\n",
    "display(crime_trend_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3870c44e-57a6-44d8-a0a8-2642376c7256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using plotly, I create a line chart showing the trend of the top 10 crimes over the 5 year period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8edd6730-e8a9-4b5d-b59f-7768c2ec8163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Convert Year column to an integer type (if necessary)\n",
    "crime_trend_pd[\"Year\"] = crime_trend_pd[\"Year\"].astype(int)\n",
    "\n",
    "# Create a line chart for crime trends\n",
    "fig = px.line(\n",
    "    crime_trend_pd,\n",
    "    x=\"Year\",\n",
    "    y=\"count\",\n",
    "    color=\"Crm Cd Desc\",\n",
    "    title=\"Crime Trends Over Time (2020-2024)\",\n",
    "    labels={\"count\": \"Number of Crimes\", \"Year\": \"Year\", \"Crm Cd Desc\": \"Crime Type\"},\n",
    "    markers=True\n",
    ")\n",
    "\n",
    "# Improve layout to ensure clear year labeling\n",
    "fig.update_layout(\n",
    "    xaxis=dict(dtick=1),  # Ensure year labels are displayed properly\n",
    "    yaxis_title=\"Number of Occurrences\",\n",
    "    xaxis_title=\"Year\"\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f2dd0bb-388e-4b9b-8889-f169127a8faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next I want to see the top 10 crime of each year, and see if they are identical or each year have different top 10 crimes. So first, i computed the top 10 crimes of each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7207e733-a2d8-4a17-8894-0eea91d1f6b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, rank\n",
    "\n",
    "# Define a window function to rank crimes within each year\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(col(\"count\").desc())\n",
    "\n",
    "# Rank crimes per year\n",
    "crime_trend_ranked = crime_trend_df.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Keep only the top 10 crimes per year\n",
    "top_crimes_per_year = crime_trend_ranked.filter(col(\"rank\") <= 10)\n",
    "\n",
    "# Convert to Pandas for Plotly\n",
    "top_crimes_per_year_pd = top_crimes_per_year.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba5ad74d-20a0-438f-8f37-6c6799a5f91e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, I visualize the top 10 crime of each year using separate bar charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad49daf6-9102-4131-8163-b7bb5b853c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Get unique years\n",
    "unique_years = sorted(top_crimes_per_year_pd[\"Year\"].unique())\n",
    "\n",
    "# Create a subplot with a row for each year\n",
    "fig = make_subplots(\n",
    "    rows=len(unique_years), \n",
    "    cols=1, \n",
    "    subplot_titles=[f\"Top 10 Crimes in {year}\" for year in unique_years]\n",
    ")\n",
    "\n",
    "# Assign consistent colors for each crime type\n",
    "color_map = {crime: px.colors.qualitative.Dark24[i % 24] for i, crime in enumerate(top_crimes_per_year_pd[\"Crm Cd Desc\"].unique())}\n",
    "\n",
    "# Add bar charts for each year, making sure to sort within each year before plotting\n",
    "for i, year in enumerate(unique_years):\n",
    "    year_df = top_crimes_per_year_pd[top_crimes_per_year_pd[\"Year\"] == year]\n",
    "    \n",
    "    # Sort the top 10 crimes within each year by count (descending) before plotting\n",
    "    year_df = year_df.sort_values(by=\"count\", ascending=True)  # Important: ascending=True for horizontal bars\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=year_df[\"count\"],\n",
    "            y=year_df[\"Crm Cd Desc\"],\n",
    "            orientation='h',\n",
    "            marker=dict(color=[color_map[crime] for crime in year_df[\"Crm Cd Desc\"]]),  # Assign consistent colors\n",
    "            name=f\"{year}\"\n",
    "        ),\n",
    "        row=i+1, col=1\n",
    "    )\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(\n",
    "    title=\"Top 10 Most Reported Crimes Per Year\",\n",
    "    xaxis_title=\"Number of Crimes\",\n",
    "    yaxis_title=\"Crime Type\",\n",
    "    height=350 * len(unique_years),  # Dynamically adjust height\n",
    "    showlegend=False  # Hide repeated legends\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80a0eb7f-cd28-4ecf-b0d5-7b8f83be3772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, I want to create a table that could show details of each crime type, including the total occurence, occurence each year, most occurred area, most common victim sex and victim race. I used \"THEFT OF IDENTITY\" as the example. But simply by changing the crime_type in step 1, I can access to the details of each crime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17d9364c-efc1-4907-94f3-34a680e3d5b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, desc\n",
    "import pandas as pd\n",
    "\n",
    "# 🔹 Step 1: Define the Crime Type\n",
    "crime_type = \"THEFT OF IDENTITY\"  # 🔹 Change this to analyze any crime\n",
    "\n",
    "# 🔹 Step 2: Filter dataset for the selected crime type\n",
    "crime_specific_df = crime_df.filter(col(\"Crm Cd Desc\") == crime_type)\n",
    "\n",
    "# 🔹 Step 3: Compute Metrics\n",
    "# 1️⃣ Total occurrences overall\n",
    "total_occurrences = crime_specific_df.count()\n",
    "\n",
    "# 2️⃣ Occurrences per year\n",
    "yearly_counts_df = (\n",
    "    crime_specific_df.groupBy(\"Year\")\n",
    "    .agg(count(\"*\").alias(\"count\"))\n",
    "    .orderBy(\"Year\")\n",
    ")\n",
    "\n",
    "# Convert yearly counts to dictionary for easier access\n",
    "yearly_counts_dict = {row[\"Year\"]: row[\"count\"] for row in yearly_counts_df.collect()}\n",
    "\n",
    "# 3️⃣ Most affected area\n",
    "most_affected_area = (\n",
    "    crime_specific_df.groupBy(\"AREA NAME\")\n",
    "    .agg(count(\"*\").alias(\"count\"))\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .limit(1)\n",
    ")\n",
    "\n",
    "most_affected_area = most_affected_area.collect()[0][\"AREA NAME\"] if most_affected_area.count() > 0 else \"N/A\"\n",
    "\n",
    "# 4️⃣ Most frequent victim racial identity\n",
    "most_common_race = (\n",
    "    crime_specific_df.groupBy(\"Vict Descent\")\n",
    "    .agg(count(\"*\").alias(\"count\"))\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .limit(1)\n",
    ")\n",
    "\n",
    "most_common_race = most_common_race.collect()[0][\"Vict Descent\"] if most_common_race.count() > 0 else \"N/A\"\n",
    "\n",
    "# 5️⃣ Most frequent victim sex identity\n",
    "most_common_sex = (\n",
    "    crime_specific_df.groupBy(\"Vict Sex\")\n",
    "    .agg(count(\"*\").alias(\"count\"))\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .limit(1)\n",
    ")\n",
    "\n",
    "most_common_sex = most_common_sex.collect()[0][\"Vict Sex\"] if most_common_sex.count() > 0 else \"N/A\"\n",
    "\n",
    "# 🔹 Step 4: Create the Summary Table\n",
    "crime_summary = {\n",
    "    \"Crime Type\": [crime_type],\n",
    "    \"Total Occurrences\": [total_occurrences],\n",
    "    \"2020 Occurrences\": [yearly_counts_dict.get(2020, 0)],\n",
    "    \"2021 Occurrences\": [yearly_counts_dict.get(2021, 0)],\n",
    "    \"2022 Occurrences\": [yearly_counts_dict.get(2022, 0)],\n",
    "    \"2023 Occurrences\": [yearly_counts_dict.get(2023, 0)],\n",
    "    \"2024 Occurrences\": [yearly_counts_dict.get(2024, 0)],\n",
    "    \"Most Affected Area\": [most_affected_area],\n",
    "    \"Most Common Victim Race\": [most_common_race],\n",
    "    \"Most Common Victim Sex\": [most_common_sex],\n",
    "}\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "crime_summary_df = pd.DataFrame(crime_summary)\n",
    "\n",
    "# 🔹 Step 5: Display the Table (Use the appropriate display function)\n",
    "display(crime_summary_df)  # Use this in Databricks to display the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "368b67c8-cf0c-48c6-93d7-d0cf3915a12f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This cell loads a CSV file from the Databricks FileStore into a Spark DataFrame.\n",
    "\n",
    "1. **Defining File Path**  \n",
    "   - The `folder_path` variable specifies the directory containing the CSV files.\n",
    "\n",
    "2. **CSV Options**  \n",
    "   - `infer_schema = \"false\"` prevents Spark from automatically inferring data types.  \n",
    "   - `first_row_is_header = \"false\"` indicates that the first row is not treated as column headers.  \n",
    "   - `delimiter = \",\"` specifies that the CSV file uses commas to separate values.\n",
    "\n",
    "3. **Loading the CSV File into a DataFrame**  \n",
    "   - `spark.read.format(\"csv\")` specifies that the file format is CSV.  \n",
    "   - `.option(\"inferSchema\", infer_schema)` prevents automatic type inference.  \n",
    "   - `.option(\"header\", first_row_is_header)` ensures Spark does not treat the first row as column names.  \n",
    "   - `.option(\"sep\", delimiter)` sets the delimiter to a comma.  \n",
    "   - `.load(folder_path)` reads the CSV data into a Spark DataFrame.\n",
    "\n",
    "4. **Extracting Column Names from the First Row**  \n",
    "   - `anoop_df.first()` retrieves the first row of the DataFrame, assuming it contains column headers.  \n",
    "   - `anoop_df.filter(df[\"_c0\"] != new_header[0])` removes the first row, as it is used for column names.\n",
    "\n",
    "5. **Renaming Columns**  \n",
    "   - `anoop_df.toDF(*[str(col) for col in new_header])` renames the columns using the extracted header values.\n",
    "\n",
    "6. **Displaying the Data**  \n",
    "   - `display(anoop_df)` prints the DataFrame in Databricks, showing a preview of the data.\n",
    "\n",
    "This approach ensures the dataset is loaded correctly with meaningful column names instead of default `_c0`, `_c1`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"/FileStore/tables/\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\" \n",
    "\n",
    "anoop_df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(folder_path)\n",
    "\n",
    "# Extract the first row as header\n",
    "new_header = anoop_df.first()  # Get first row\n",
    "anoop_df = anoop_df.filter(anoop_df[\"_c0\"] != new_header[0])  # Remove first row\n",
    "\n",
    "# Rename columns using extracted header\n",
    "anoop_df = anoop_df.toDF(*[str(col) for col in new_header])\n",
    "\n",
    "\n",
    "display(anoop_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cd71177-fee4-43e0-b54e-a972d91ed0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This cell processes the \"TIME OCC\" (Time of Occurrence) column to ensure it contains only valid numerical values in the HHMM (hour-minute) format.\n",
    "\n",
    "1. **Importing Required Functions**\n",
    "   - The `pyspark.sql.functions` module provides functions for data manipulation.\n",
    "   - `col`: Used to refer to DataFrame columns.\n",
    "   - `regexp_extract`: Extracts substrings from a column using regular expressions.\n",
    "   - Other functions (`count, when, floor, concat, lit, lpad, to_date, dayofweek, to_timestamp, date_format`) are imported but not yet used in this block.\n",
    "   - `plotly.express as px` is imported for visualization purposes but not used in this step.\n",
    "\n",
    "2. **Extracting Valid Time Values**\n",
    "   - `anoop_df.withColumn(\"TIME OCC\", ...)` creates a new DataFrame (`df_time`) where the \"TIME OCC\" column is processed.\n",
    "   - `col(\"TIME OCC\").cast(\"string\")` ensures the column is treated as a string.\n",
    "   - `regexp_extract(col(\"TIME OCC\"), r\"(\\d{3,4})\", 0)`:  \n",
    "     - This extracts 3 or 4-digit numbers from \"TIME OCC\".\n",
    "     - The regular expression `(\\d{3,4})` captures valid time values in HHMM format.\n",
    "     - This helps clean the dataset and remove any non-numeric values or incorrectly formatted time values.\n",
    "\n",
    "By performing this transformation, we ensure that the \"TIME OCC\" column contains only properly formatted time values, which is essential for time-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d19398f-9278-4df7-9ee6-c5b16903ed4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, regexp_extract, when, floor, concat, lit, lpad, to_date, dayofweek,to_timestamp, date_format\n",
    "import plotly.express as px\n",
    "\n",
    "# Ensure TIME OCC contains only numbers, extracting valid HHMM values (3 or 4-digit numbers)\n",
    "df_time = anoop_df.withColumn(\"TIME OCC\", regexp_extract(col(\"TIME OCC\").cast(\"string\"), r\"(\\d{3,4})\", 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dd5d8a9-737d-4b2d-aa9b-57db50ce2a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This cell ensures that the \"TIME OCC\" column contains only valid integer values by handling invalid entries and null values.\n",
    "\n",
    "1. **Safely Converting \"TIME OCC\" to an Integer**\n",
    "   - `df_time.withColumn(\"TIME OCC\", when(col(\"TIME OCC\") != \"\", col(\"TIME OCC\").cast(\"int\")).otherwise(None))`:\n",
    "     - Checks if the column is not empty (`col(\"TIME OCC\") != \"\"`).\n",
    "     - Converts non-empty values to integers using `.cast(\"int\")`.\n",
    "     - If the value is empty or invalid, it is replaced with `None` (NULL), ensuring that invalid data does not cause errors.\n",
    "\n",
    "2. **Handling Missing Values**\n",
    "   - `df_time.dropna(subset=[\"TIME OCC\"])`:\n",
    "     - Removes rows where \"TIME OCC\" is NULL.\n",
    "     - This ensures that all time values are valid and prevents issues in subsequent time-based analysis.\n",
    "\n",
    "By performing this transformation, we ensure that:\n",
    "- All values in \"TIME OCC\" are integers.\n",
    "- Any invalid or empty values are handled safely.\n",
    "- The dataset remains clean for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a80dd02-b791-40f5-9cd7-748efea32edc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert TIME OCC to integer safely (invalid values become NULL)\n",
    "df_time = df_time.withColumn(\"TIME OCC\", when(col(\"TIME OCC\") != \"\", col(\"TIME OCC\").cast(\"int\")).otherwise(None))\n",
    "# Replace NULL values with a default safe time (e.g., 0000 for midnight)\n",
    "df_time = df_time.dropna(subset = [\"TIME OCC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1800910c-99f1-4ad1-a300-616b90e9d47a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This cell processes the \"TIME OCC\" column to extract hour-based information and analyze crime distribution across different hours of the day.\n",
    "\n",
    "1. **Extracting the Hour from \"TIME OCC\" (HHMM Format)**\n",
    "   - `df_time.withColumn(\"Hour\", floor(col(\"TIME OCC\") / 100).cast(\"int\"))`:\n",
    "     - Since \"TIME OCC\" is in HHMM format (e.g., 2130 for 9:30 PM), dividing by 100 extracts only the hour.\n",
    "     - `floor()` ensures that only the integer part (hour) remains.\n",
    "     - `.cast(\"int\")` converts the extracted value to an integer for further processing.\n",
    "\n",
    "2. **Formatting Hours with Leading Zeros**\n",
    "   - `df_hour.withColumn(\"Hour Formatted\", lpad(col(\"Hour\").cast(\"string\"), 2, \"0\"))`:\n",
    "     - Ensures that single-digit hours (e.g., 9) are displayed as two-digit values (e.g., \"09\").\n",
    "     - `lpad()` adds a leading zero where necessary.\n",
    "\n",
    "3. **Creating an Hourly Time Bin Label**\n",
    "   - `df_hour.withColumn(\"Hour Bin\", concat(col(\"Hour Formatted\").cast(\"string\"), lit(\":00 - \"), col(\"Hour Formatted\").cast(\"string\"), lit(\":59\")))`:\n",
    "     - Creates a time range for each hour, such as \"00:00 - 00:59\" or \"23:00 - 23:59\".\n",
    "     - `concat()` combines formatted hour strings with text labels to create readable bin labels.\n",
    "\n",
    "4. **Counting Crimes per Hour**\n",
    "   - `crime_by_hour = df_hour.groupBy(\"Hour Bin\").agg(count(\"*\").alias(\"Crime_Count\")).orderBy(\"Hour Bin\")`:\n",
    "     - Groups crimes based on hourly time bins.\n",
    "     - Uses `count(\"*\")` to count the number of crimes in each time bin.\n",
    "     - `orderBy(\"Hour Bin\")` ensures that the results are sorted in chronological order.\n",
    "\n",
    "This transformation enables analysis of crime trends across different hours of the day, helping to identify peak crime periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd6bea4-72cb-418d-9b62-3035f5156a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract hour from TIME OCC (HHMM format)\n",
    "df_hour = df_time.withColumn(\"Hour\", floor(col(\"TIME OCC\") / 100).cast(\"int\"))\n",
    "\n",
    "# Ensure hours are formatted correctly with leading zeros\n",
    "df_hour = df_hour.withColumn(\"Hour Formatted\", lpad(col(\"Hour\").cast(\"string\"), 2, \"0\"))\n",
    "\n",
    "# Create a bin label (e.g., \"00:00-00:59\", \"01:00-01:59\", ...)\n",
    "df_hour = df_hour.withColumn(\"Hour Bin\", concat(\n",
    "    col(\"Hour Formatted\").cast(\"string\"),\n",
    "    lit(\":00 - \"),\n",
    "    col(\"Hour Formatted\").cast(\"string\"),\n",
    "    lit(\":59\")\n",
    "))\n",
    "\n",
    "# Count crimes per hour\n",
    "crime_by_hour = df_hour.groupBy(\"Hour Bin\").agg(count(\"*\").alias(\"Crime_Count\")).orderBy(\"Hour Bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90bf8271-5fd0-4c23-9887-c8a020b4adf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This cell displays the summarized crime count per hour using the Databricks `display()` function.\n",
    "\n",
    "1. **Displaying the Results**\n",
    "   - `display(crime_by_hour)`:  \n",
    "     - This function renders the `crime_by_hour` DataFrame as an interactive table in Databricks.\n",
    "     - The table contains two columns:\n",
    "       - **Hour Bin**: Represents the time range (e.g., \"00:00 - 00:59\", \"01:00 - 01:59\").\n",
    "       - **Crime_Count**: The total number of crimes that occurred within that hour bin.\n",
    "\n",
    "2. **Insights from the Table**\n",
    "   - The table allows easy identification of peak crime hours.\n",
    "   - Crime counts are highest during midday (e.g., 12:00 - 12:59) and lower in the early morning hours.\n",
    "   - This information can be useful for law enforcement and city planning to allocate resources effectively.\n",
    "\n",
    "The `display()` function in Databricks is particularly useful for interactive exploration, filtering, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6cda275-9a97-4c08-8b4c-b3b3404f7969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display results\n",
    "display(crime_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb781612-e3e0-448d-927f-2bcf685cc4ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This cell creates an interactive bar chart using Plotly to visualize crime frequency by hour.\n",
    "\n",
    "1. **Converting DataFrame to Pandas**\n",
    "   - `crime_by_hour_pd = crime_by_hour.toPandas()`:\n",
    "     - Converts the Spark DataFrame into a Pandas DataFrame.\n",
    "     - This step is necessary because Plotly requires Pandas for visualization in Databricks.\n",
    "\n",
    "2. **Creating a Bar Chart with Plotly**\n",
    "   - `px.bar(...)` generates a bar chart with:\n",
    "     - `x=\"Hour Bin\"`: The hourly time bins (e.g., \"00:00 - 00:59\").\n",
    "     - `y=\"Crime_Count\"`: The total number of crimes in each hour.\n",
    "     - `title=\"Crime Frequency by Hour of the Day\"`: The title of the chart.\n",
    "     - `labels={\"Hour Bin\": \"Hour of the Day\", \"Crime_Count\": \"Crime Frequency\"}`: Axis labels for clarity.\n",
    "     - `text_auto=True`: Displays crime counts on top of each bar.\n",
    "\n",
    "3. **Customizing the Chart Layout**\n",
    "   - `fig.update_layout(...)` applies several formatting enhancements:\n",
    "     - `xaxis_tickangle=-45`: Rotates x-axis labels by 45 degrees for better readability.\n",
    "     - `bargap=0.1`: Adds spacing between bars.\n",
    "     - `template=\"plotly_dark\"`: Uses a dark theme, which is well-suited for Databricks.\n",
    "\n",
    "4. **Displaying the Chart**\n",
    "   - `fig.show()`: Renders the interactive visualization.\n",
    "\n",
    "### **Insights from the Bar Chart**\n",
    "- The highest crime occurrences are around **midday (12:00 - 12:59)**.\n",
    "- Crime rates are significantly lower during early morning hours.\n",
    "- The chart provides a clear hour-wise crime distribution, helping in strategic law enforcement and resource allocation.\n",
    "\n",
    "This interactive visualization allows further exploration, such as zooming and hovering over bars to see precise values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dc39d18-928f-4233-8d4f-f1d9db008b77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If you want to visualize in Databricks, convert to Pandas and use matplotlib\n",
    "crime_by_hour_pd = crime_by_hour.toPandas()\n",
    "\n",
    "# Plot interactive bar chart using Plotly\n",
    "fig = px.bar(\n",
    "    crime_by_hour_pd,\n",
    "    x=\"Hour Bin\",\n",
    "    y=\"Crime_Count\",\n",
    "    title=\"Crime Frequency by Hour of the Day\",\n",
    "    labels={\"Hour Bin\": \"Hour of the Day\", \"Crime_Count\": \"Crime Frequency\"},\n",
    "    text_auto=True,  # Display count on bars\n",
    ")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45,  # Rotate x-axis labels for readability\n",
    "    bargap=0.1,  # Add some space between bars\n",
    "    template=\"plotly_dark\"  # Dark theme for Databricks compatibility\n",
    ")\n",
    "\n",
    "# Show interactive Plotly graph\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "154ba191-0cf0-43ed-8114-dd71e32f3949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This cell processes the \"DATE OCC\" column to extract the day of the week from crime occurrence dates.\n",
    "\n",
    "1. **Ensuring \"DATE OCC\" is a String**\n",
    "   - `anoop_df.withColumn(\"DATE OCC\", col(\"DATE OCC\").cast(\"string\"))`:\n",
    "     - Ensures that the \"DATE OCC\" column is treated as a string before conversion.\n",
    "     - This step prevents potential formatting issues when converting to a timestamp.\n",
    "\n",
    "2. **Converting \"DATE OCC\" to a Timestamp**\n",
    "   - `df_day.withColumn(\"Date\", to_timestamp(col(\"DATE OCC\"), \"MM/dd/yyyy hh:mm:ss a\"))`:\n",
    "     - Converts the date string into a Spark timestamp format.\n",
    "     - The provided format `\"MM/dd/yyyy hh:mm:ss a\"` matches the structure of the dataset.\n",
    "\n",
    "3. **Extracting the Day of the Week**\n",
    "   - `df_day.withColumn(\"DayOfWeekNum\", dayofweek(col(\"Date\")))`:\n",
    "     - Uses Spark's `dayofweek()` function to extract the day number (1 to 7).\n",
    "     - In Spark, **1 = Sunday** and **7 = Saturday**.\n",
    "\n",
    "4. **Mapping Numeric Days to Actual Names**\n",
    "   - `.withColumn(\"DayOfWeek\", when(col(\"DayOfWeekNum\") == 1, \"Sunday\")...otherwise(None))`:\n",
    "     - Converts numeric day representations into human-readable day names.\n",
    "     - This makes the dataset easier to interpret and analyze.\n",
    "\n",
    "5. **Displaying Sample Data**\n",
    "   - `df_day.select(\"DATE OCC\", \"Date\", \"DayOfWeekNum\", \"DayOfWeek\").show(10, truncate=False)`:\n",
    "     - Selects relevant columns and displays the first 10 rows for verification.\n",
    "     - The output confirms that each date is correctly mapped to its corresponding weekday.\n",
    "\n",
    "### **Insights from the Output**\n",
    "- The transformation successfully maps dates to their respective weekday names.\n",
    "- This information is useful for analyzing crime trends by day of the week.\n",
    "- For example, if crime rates are higher on weekends, law enforcement agencies can allocate more resources accordingly.\n",
    "\n",
    "This step is crucial for understanding temporal patterns in crime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44cddf43-b28b-4360-8270-4b4bef4bcbcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure DATE OCC is treated as a string before conversion\n",
    "df_day = anoop_df.withColumn(\"DATE OCC\", col(\"DATE OCC\").cast(\"string\"))\n",
    "\n",
    "# Convert to timestamp using the detected format\n",
    "df_day = df_day.withColumn(\"Date\", to_timestamp(col(\"DATE OCC\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "# Extract day of the week (1 = Sunday, 7 = Saturday)\n",
    "df_day = df_day.withColumn(\"DayOfWeekNum\", dayofweek(col(\"Date\")))\n",
    "\n",
    "# Map numeric days to actual names\n",
    "df_day = df_day.withColumn(\n",
    "    \"DayOfWeek\",\n",
    "    when(col(\"DayOfWeekNum\") == 1, \"Sunday\")\n",
    "    .when(col(\"DayOfWeekNum\") == 2, \"Monday\")\n",
    "    .when(col(\"DayOfWeekNum\") == 3, \"Tuesday\")\n",
    "    .when(col(\"DayOfWeekNum\") == 4, \"Wednesday\")\n",
    "    .when(col(\"DayOfWeekNum\") == 5, \"Thursday\")\n",
    "    .when(col(\"DayOfWeekNum\") == 6, \"Friday\")\n",
    "    .when(col(\"DayOfWeekNum\") == 7, \"Saturday\")\n",
    ")\n",
    "\n",
    "# Show results to verify\n",
    "df_day.select(\"DATE OCC\", \"Date\", \"DayOfWeekNum\", \"DayOfWeek\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d81d79be-2795-48d9-9165-8495be4aa91d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This cell aggregates crime data by the day of the week and visualizes it using an interactive bar chart.\n",
    "\n",
    "1. **Counting Crimes for Each Day of the Week**\n",
    "   - `crime_by_day = df_day.groupBy(\"DayOfWeek\").agg(count(\"*\").alias(\"Crime_Count\"))`:\n",
    "     - Groups the dataset by \"DayOfWeek\" (e.g., Monday, Tuesday).\n",
    "     - Uses `count(\"*\")` to count the number of crimes occurring on each day.\n",
    "\n",
    "   - `.orderBy(\"DayOfWeekNum\")`:\n",
    "     - Ensures that the days appear in the correct weekday order (Monday–Sunday).\n",
    "\n",
    "2. **Dropping Numeric Day Column (Cleanup)**\n",
    "   - `crime_by_day = crime_by_day.drop(\"DayOfWeekNum\")`:\n",
    "     - Removes the numeric representation of days since it's no longer needed for visualization.\n",
    "\n",
    "3. **Converting DataFrame to Pandas for Visualization**\n",
    "   - `crime_by_day_pd = crime_by_day.toPandas()`:\n",
    "     - Converts the Spark DataFrame into a Pandas DataFrame for compatibility with Plotly.\n",
    "\n",
    "4. **Creating a Bar Chart with Plotly**\n",
    "   - `px.bar(...)` generates a bar chart with:\n",
    "     - `x=\"DayOfWeek\"`: The weekday names (e.g., Monday, Tuesday).\n",
    "     - `y=\"Crime_Count\"`: The total number of crimes on each day.\n",
    "     - `title=\"Crime Frequency by Day of the Week\"`: The chart title.\n",
    "     - `labels={\"DayOfWeek\": \"Day of the Week\", \"Crime_Count\": \"Crime Frequency\"}`: Provides axis labels.\n",
    "     - `text_auto=True`: Displays crime counts on top of bars.\n",
    "     - `color=\"DayOfWeek\"`: Uses different colors for each day for better visualization.\n",
    "\n",
    "5. **Customizing Chart Layout**\n",
    "   - `fig.update_layout(...)` applies additional formatting:\n",
    "     - `xaxis_tickangle=-45`: Rotates x-axis labels by 45 degrees for better readability.\n",
    "     - `bargap=0.1`: Adds spacing between bars.\n",
    "     - `template=\"plotly_dark\"`: Uses a dark theme for Databricks compatibility.\n",
    "\n",
    "6. **Displaying the Chart**\n",
    "   - `fig.show()`: Renders the interactive visualization.\n",
    "\n",
    "### **Insights from the Output**\n",
    "- **Friday** has the highest number of reported crimes.\n",
    "- **Tuesday and Sunday** have slightly lower crime rates compared to other days.\n",
    "- The consistent crime distribution throughout the week suggests no extreme weekday patterns, but weekends might see an increase.\n",
    "\n",
    "This visualization helps law enforcement and policymakers understand when crimes are most frequent, assisting in strategic planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28afa477-31eb-4fb9-a6ed-bf3b88c492a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count crimes per day of the week\n",
    "crime_by_day = df_day.groupBy(\"DayOfWeek\", \"DayOfWeekNum\") \\\n",
    "    .agg(count(\"*\").alias(\"Crime_Count\")) \\\n",
    "    .orderBy(\"DayOfWeekNum\")  # Ensures correct weekday order (Monday–Sunday)\n",
    "\n",
    "# Drop numeric day column before visualization\n",
    "crime_by_day = crime_by_day.drop(\"DayOfWeekNum\")\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "crime_by_day_pd = crime_by_day.toPandas()\n",
    "\n",
    "# Plot interactive bar chart using Plotly\n",
    "fig = px.bar(\n",
    "    crime_by_day_pd,\n",
    "    x=\"DayOfWeek\",\n",
    "    y=\"Crime_Count\",\n",
    "    title=\"Crime Frequency by Day of the Week\",\n",
    "    labels={\"DayOfWeek\": \"Day of the Week\", \"Crime_Count\": \"Crime Frequency\"},\n",
    "    text_auto=True,  # Display count on bars\n",
    "    color=\"DayOfWeek\",  # Different colors for each day\n",
    ")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45,  # Rotate labels for readability\n",
    "    bargap=0.1,  # Add space between bars\n",
    "    template=\"plotly_dark\"  # Dark theme for Databricks compatibility\n",
    ")\n",
    "\n",
    "# Show interactive Plotly graph\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fee01c5-0c6c-4840-a0a1-b8c0ced9342b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This cell extracts the year and month from the \"DATE OCC\" column and aggregates crime counts per month and year.\n",
    "\n",
    "1. **Extracting Year and Month from \"DATE OCC\"**\n",
    "   - `df_day.withColumn(\"Year\", date_format(col(\"Date\"), \"yyyy\").cast(\"int\"))`:\n",
    "     - Extracts the **year** from the \"Date\" column and converts it into an integer.\n",
    "\n",
    "   - `df_day.withColumn(\"Month\", date_format(col(\"Date\"), \"MMMM\"))`:\n",
    "     - Extracts the **full month name** (e.g., \"January\", \"February\") for better readability in visualizations.\n",
    "\n",
    "   - `df_day.withColumn(\"MonthNum\", date_format(col(\"Date\"), \"MM\").cast(\"int\"))`:\n",
    "     - Extracts the **numeric month** (1–12) for sorting purposes.\n",
    "     - This helps maintain chronological order when analyzing trends.\n",
    "\n",
    "2. **Counting Crimes per Month and Year**\n",
    "   - `crime_trends = df_day.groupBy(\"Year\", \"Month\", \"MonthNum\").agg(count(\"*\").alias(\"Crime_Count\"))`:\n",
    "     - Groups crimes by **year and month**.\n",
    "     - Counts the total number of crimes in each month.\n",
    "\n",
    "   - `.orderBy(\"Year\", \"MonthNum\")`:\n",
    "     - Ensures that months are displayed in proper chronological order (January → December) for each year.\n",
    "\n",
    "3. **Dropping \"MonthNum\" After Sorting**\n",
    "   - `crime_trends = crime_trends.drop(\"MonthNum\")`:\n",
    "     - The \"MonthNum\" column is no longer needed after sorting.\n",
    "     - This keeps the dataset clean for visualization.\n",
    "\n",
    "### **Why This Step is Important?**\n",
    "- It allows analysis of crime trends over time (monthly and yearly patterns).\n",
    "- Helps in identifying **seasonal variations** or **anomalies** in crime rates.\n",
    "- Prepares the data for visualization in the next steps.\n",
    "\n",
    "By structuring the data this way, we can generate time-series plots to see how crime rates fluctuate over different months and years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00d73e10-78c3-441a-be6b-7354182c7a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract Year and Month from DATE OCC\n",
    "df_day = df_day.withColumn(\"Year\", date_format(col(\"Date\"), \"yyyy\").cast(\"int\"))\n",
    "df_day = df_day.withColumn(\"Month\", date_format(col(\"Date\"), \"MMMM\"))  # Full month name\n",
    "df_day = df_day.withColumn(\"MonthNum\", date_format(col(\"Date\"), \"MM\").cast(\"int\"))  # Numeric for sorting\n",
    "\n",
    "# Count crimes per month and year\n",
    "crime_trends = df_day.groupBy(\"Year\", \"Month\", \"MonthNum\") \\\n",
    "    .agg(count(\"*\").alias(\"Crime_Count\")) \\\n",
    "    .orderBy(\"Year\", \"MonthNum\")  # Ensure proper chronological order\n",
    "\n",
    "# Drop MonthNum after sorting (not needed for visualization)\n",
    "crime_trends = crime_trends.drop(\"MonthNum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b607df9-cb2e-43db-b033-0070c0b6dba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This cell displays the aggregated crime count per month and year using the Databricks `display()` function.\n",
    "\n",
    "1. **Displaying the Results**\n",
    "   - `display(crime_trends)`:  \n",
    "     - Shows the `crime_trends` DataFrame as an interactive table in Databricks.\n",
    "     - The table includes:\n",
    "       - **Year**: The year in which the crime occurred.\n",
    "       - **Month**: The month name (e.g., \"January\", \"February\").\n",
    "       - **Crime_Count**: The total number of crimes reported in that month.\n",
    "\n",
    "2. **Insights from the Table**\n",
    "   - Crime numbers vary month-to-month but generally stay within a certain range.\n",
    "   - The data is **chronologically sorted** (January → December for each year).\n",
    "   - This allows for **trend analysis** to identify seasonal variations in crime.\n",
    "\n",
    "This table serves as the foundation for visualizing crime trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8115c5a7-16d8-4ca8-ad43-84f3a1d18adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(crime_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e64f553a-bf97-4ee3-8a09-ab934d14ce42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This cell creates an interactive **line chart** using Plotly to visualize monthly crime trends across different years.\n",
    "\n",
    "1. **Converting DataFrame to Pandas for Visualization**\n",
    "   - `crime_trends_pd = crime_trends.toPandas()`:\n",
    "     - Converts the Spark DataFrame into a Pandas DataFrame.\n",
    "     - Required because Plotly in Databricks works best with Pandas DataFrames.\n",
    "\n",
    "2. **Creating a Line Chart to Show Trends**\n",
    "   - `px.line(...)` generates a line plot with:\n",
    "     - `x=\"Month\"`: The month names on the x-axis.\n",
    "     - `y=\"Crime_Count\"`: The total crime frequency for each month.\n",
    "     - `color=\"Year\"`: Each year's trend is plotted in a separate line.\n",
    "     - `title=\"Crime Trends Over Time (Monthly Breakdown)\"`: Adds a meaningful title.\n",
    "     - `labels={\"Month\": \"Month\", \"Crime_Count\": \"Crime Frequency\", \"Year\": \"Year\"}`:\n",
    "       - Assigns clear labels to axes and legend.\n",
    "     - `markers=True`: Displays data points on the line for better visibility.\n",
    "\n",
    "3. **Customizing the Chart Layout**\n",
    "   - `fig.update_layout(...)` applies additional formatting:\n",
    "     - `xaxis_tickangle=-45`: Rotates the month labels by **45 degrees** for better readability.\n",
    "     - `template=\"plotly_dark\"`: Uses a dark theme for Databricks compatibility.\n",
    "\n",
    "4. **Displaying the Interactive Chart**\n",
    "   - `fig.show()`: Renders the visualization.\n",
    "\n",
    "### **Insights from the Output**\n",
    "- The plot shows **seasonal crime patterns** across multiple years.\n",
    "- Certain months have noticeable dips or spikes in crime frequency.\n",
    "- Crime counts appear **relatively stable** across the years, but some years exhibit declining or increasing trends.\n",
    "- The lowest crime numbers in some years (e.g., 2024) may indicate **external factors** (e.g., policy changes, social conditions, pandemic impact).\n",
    "\n",
    "This visualization helps in **identifying patterns**, **predicting future trends**, and **assisting law enforcement planning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17892212-4c25-4e24-8069-77c8dce1ee36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas for visualization\n",
    "crime_trends_pd = crime_trends.toPandas()\n",
    "\n",
    "# Create a line chart for trends\n",
    "fig = px.line(\n",
    "    crime_trends_pd,\n",
    "    x=\"Month\",\n",
    "    y=\"Crime_Count\",\n",
    "    color=\"Year\",  # Different lines for each year\n",
    "    title=\"Crime Trends Over Time (Monthly Breakdown)\",\n",
    "    labels={\"Month\": \"Month\", \"Crime_Count\": \"Crime Frequency\", \"Year\": \"Year\"},\n",
    "    markers=True  # Adds data points on the line\n",
    ")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45,  # Rotate labels for readability\n",
    "    template=\"plotly_dark\",  # Dark theme for Databricks compatibility\n",
    ")\n",
    "\n",
    "# Show interactive Plotly graph\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "813bb6ae-1502-4e77-b414-152e34f6a3ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This cell aggregates crime data on a **yearly basis** and visualizes the overall trend using a line chart.\n",
    "\n",
    "1. **Extracting the Year from** `df_day`\n",
    "   - `df_day.withColumn(\"Year\", date_format(col(\"Date\"), \"yyyy\").cast(\"int\"))`:\n",
    "     - Extracts the **year** from the \"Date\" column.\n",
    "     - Converts it into an integer for easier filtering and sorting.\n",
    "\n",
    "2. **Filtering Data for Years 2020 - 2024**\n",
    "   - `df_filtered = df_trend.filter((col(\"Year\") >= 2020) & (col(\"Year\") <= 2024))`:\n",
    "     - Filters crime records to only include the years **2020 - 2024**.\n",
    "     - Ensures we focus on recent crime trends.\n",
    "\n",
    "3. **Counting Crimes Per Year**\n",
    "   - `crime_trends_yearly = df_filtered.groupBy(\"Year\").agg(count(\"*\").alias(\"Crime_Count\"))`:\n",
    "     - Groups the data by **year**.\n",
    "     - Counts the total number of crimes reported in each year.\n",
    "\n",
    "   - `.orderBy(\"Year\")`:\n",
    "     - Ensures that the results are **chronologically ordered**.\n",
    "\n",
    "4. **Converting DataFrame to Pandas for Visualization**\n",
    "   - `crime_trends_yearly_pd = crime_trends_yearly.toPandas()`:\n",
    "     - Converts the Spark DataFrame into a Pandas DataFrame.\n",
    "     - Required for **Plotly visualizations**.\n",
    "\n",
    "5. **Creating a Line Chart to Show Yearly Trends**\n",
    "   - `px.line(...)` generates a **line plot** with:\n",
    "     - `x=\"Year\"`: The years (2020 - 2024) on the x-axis.\n",
    "     - `y=\"Crime_Count\"`: The total crime frequency on the y-axis.\n",
    "     - `title=\"Crime Trends Over the Years (2020 - 2024)\"`: Adds a meaningful title.\n",
    "     - `labels={\"Year\": \"Year\", \"Crime_Count\": \"Crime Frequency\"}`:\n",
    "       - Ensures clear axis labels.\n",
    "     - `markers=True`: Displays data points on the line.\n",
    "     - `line_shape=\"linear\"`: Ensures a straight-line connection between data points.\n",
    "\n",
    "6. **Customizing the Chart Layout**\n",
    "   - `fig.update_layout(...)` applies additional formatting:\n",
    "     - `template=\"plotly_dark\"`: Uses a dark theme for Databricks compatibility.\n",
    "     - `xaxis=dict(tickmode=\"linear\", dtick=1)`: Ensures **each year is labeled** on the x-axis.\n",
    "\n",
    "7. **Displaying the Interactive Chart**\n",
    "   - `fig.show()`: Renders the visualization.\n",
    "\n",
    "### **Insights from the Output**\n",
    "- **Crime rates increased from 2020 to 2022**, indicating a **rising trend**.\n",
    "- **2023 saw a slight decline**, suggesting possible external factors (e.g., policy changes, law enforcement measures).\n",
    "- **2024 shows a sharp drop**, but this might be due to incomplete data for the year or other external influences.\n",
    "- This visualization helps in **understanding long-term trends** and predicting future crime patterns.\n",
    "\n",
    "By visualizing yearly crime trends, law enforcement agencies can **analyze patterns**, **allocate resources effectively**, and **identify potential causes** of crime fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c705025a-7010-432e-9922-93ac7c9890bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract Year from DATE OCC\n",
    "df_trend = df_day.withColumn(\"Year\", date_format(col(\"Date\"), \"yyyy\").cast(\"int\"))\n",
    "\n",
    "# Filter data for years 2020-2024\n",
    "df_filtered = df_trend.filter((col(\"Year\") >= 2020) & (col(\"Year\") <= 2024))\n",
    "\n",
    "# Count crimes per year\n",
    "crime_trends_yearly = df_filtered.groupBy(\"Year\") \\\n",
    "    .agg(count(\"*\").alias(\"Crime_Count\")) \\\n",
    "    .orderBy(\"Year\")  # Ensure chronological order\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "crime_trends_yearly_pd = crime_trends_yearly.toPandas()\n",
    "\n",
    "# Create a line chart for yearly trends\n",
    "fig = px.line(\n",
    "    crime_trends_yearly_pd,\n",
    "    x=\"Year\",\n",
    "    y=\"Crime_Count\",\n",
    "    title=\"Crime Trends Over the Years (2020 - 2024)\",\n",
    "    labels={\"Year\": \"Year\", \"Crime_Count\": \"Crime Frequency\"},\n",
    "    markers=True,  # Adds data points on the line\n",
    "    line_shape=\"linear\"\n",
    ")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    template=\"plotly_dark\",  \n",
    "    xaxis=dict(tickmode=\"linear\", dtick=1)  # Ensure each year is labeled\n",
    ")\n",
    "\n",
    "# Show interactive Plotly graph\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb39450a-26c7-41e4-ad1d-99461a61d601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d01e21f-80f1-4f34-8530-5a3fc5f250d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# This line uses Databricks' magic command %pip to install the folium Python library within the current notebook's environment. folium is a library used for creating interactive maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a53604f-01ae-4b7b-85c0-ef391a268f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folder_path = \"/FileStore/tables/\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\" \n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(folder_path)\n",
    "\n",
    "# Extract the first row as header\n",
    "new_header = df.first()  # Get first row\n",
    "df = df.filter(df[\"_c0\"] != new_header[0])  # Remove first row\n",
    "\n",
    "# Rename columns using extracted header\n",
    "df = df.toDF(*[str(col) for col in new_header])\n",
    "\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2a2e120-a943-4c33-af0d-99994fc8aa80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "File Location and Type: It defines the path to the CSV file (file_location) and specifies the file type as \"csv\" (file_type).\n",
    "CSV Options: It sets options for reading the CSV: inferSchema is set to \"true\" to automatically determine the data types of each column, first_row_is_header is set to \"true\" to treat the first row as column headers, and delimiter is set to \",\" to specify the comma as the column separator.\n",
    "Reading the Data: It uses spark.read.format(file_type) to create a DataFrame reader, applies the specified options, and then loads the data from the given file_location into a DataFrame named df.\n",
    "Displaying the DataFrame: Finally, display(df) is used to display the contents of the DataFrame within the Databricks notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c93ed6-e51e-4ef7-b091-c5f30f4a52d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert Spark DataFrame to pandas\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "print(pandas_df.shape, \"Shape of the dataframe (Number of Observations, Number of Attributes)\")\n",
    "#print(pandas_df.columns)\n",
    "\n",
    "\n",
    "print(pandas_df.dtypes) # show each attribute's name and data type\n",
    "\n",
    "# Check for NaN values in each column\n",
    "nan_counts = pandas_df.isnull().sum()\n",
    "print('Number of NaN values for each variable:\\n',nan_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71435bb2-b619-4251-920d-79128f9c7196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Spark to pandas Conversion: Converts the Spark DataFrame df into a pandas DataFrame, enabling pandas-specific data manipulation.\n",
    "DataFrame Shape: Displays the dimensions of the pandas DataFrame (number of rows and columns).\n",
    "Data Types: Prints the data type of each column in the DataFrame.\n",
    "NaN Value Check: Calculates the number of missing (NaN) values for each column, and displays these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ad9da4-ca8c-49fc-9d6f-423ba3f4b790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert \"Date Rptd\" to datetime format\n",
    "pandas_df[\"Date Rptd\"] = pd.to_datetime(pandas_df[\"Date Rptd\"], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "\n",
    "# Create a new column with only the date (without time)\n",
    "pandas_df[\"Date Rptd2\"] = pandas_df[\"Date Rptd\"].dt.date\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(pandas_df[[\"Date Rptd\", \"Date Rptd2\"]].head())\n",
    "\n",
    "# Print the range of dates\n",
    "date_min = pandas_df[\"Date Rptd2\"].min()\n",
    "date_max = pandas_df[\"Date Rptd2\"].max()\n",
    "\n",
    "print(f\"Date range in 'Date Rptd2': {date_min} to {date_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5acf352-4584-49be-8dc5-96fd0a1fe097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Datetime Conversion: Converts the \"Date Rptd\" column, which contains date and time information as strings, into pandas datetime objects. The format argument specifies the expected string format.\n",
    "Date Extraction: Creates a new column \"Date Rptd2\" that contains only the date part of the \"Date Rptd\" column (without the time).\n",
    "Verification: Displays the first few rows of the \"Date Rptd\" and \"Date Rptd2\" columns to ensure the conversion and extraction were successful.\n",
    "Date Range: Determine the minimum and maximum dates in the \"Date Rptd2\" column, and prints the range of dates in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae4f60f-d308-457d-83f1-f783ba87009c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Filter data for 2020\n",
    "pandas_df_2020 = pandas_df[pandas_df[\"Date Rptd\"].dt.year == 2020]\n",
    "\n",
    "pandas_df_2020[\"LAT\"] = pd.to_numeric(pandas_df_2020[\"LAT\"], errors=\"coerce\")\n",
    "pandas_df_2020[\"LON\"] = pd.to_numeric(pandas_df_2020[\"LON\"], errors=\"coerce\")\n",
    "\n",
    "# Ensure the DataFrame contains valid coordinates\n",
    "pandas_df_2020 = pandas_df_2020.dropna(subset=[\"LAT\", \"LON\"])\n",
    "\n",
    "# Create the map using the 2020 crime data\n",
    "m = folium.Map(location=[pandas_df_2020[\"LAT\"].mean(), pandas_df_2020[\"LON\"].mean()], zoom_start=10)\n",
    "\n",
    "# Add heatmap for 2020 data\n",
    "HeatMap(pandas_df_2020[[\"LAT\", \"LON\"]].values, radius=5, blur=10).add_to(m)\n",
    "\n",
    "# Display map\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f2b748-87bf-415e-8490-b24cd904e01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter data for 2021\n",
    "pandas_df_2021 = pandas_df[pandas_df[\"Date Rptd\"].dt.year == 2021]\n",
    "\n",
    "pandas_df_2021[\"LAT\"] = pd.to_numeric(pandas_df_2021[\"LAT\"], errors=\"coerce\")\n",
    "pandas_df_2021[\"LON\"] = pd.to_numeric(pandas_df_2021[\"LON\"], errors=\"coerce\")\n",
    "# Ensure the DataFrame contains valid coordinates\n",
    "pandas_df_2021 = pandas_df_2021.dropna(subset=[\"LAT\", \"LON\"])\n",
    "\n",
    "# Create the map using the 2021 crime data\n",
    "m = folium.Map(location=[pandas_df_2021[\"LAT\"].mean(), pandas_df_2021[\"LON\"].mean()], zoom_start=10)\n",
    "\n",
    "# Add heatmap for 2021 data\n",
    "HeatMap(pandas_df_2021[[\"LAT\", \"LON\"]].values, radius=5, blur=10).add_to(m)\n",
    "\n",
    "# Display map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "803743f4-d449-4f0c-84fe-9a2c2a376763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This code block generates a heatmap visualization of crime data for the year 2021 using folium. It removes any rows that have missing (NaN) values in the \"LAT\" (latitude) or \"LON\" (longitude) columns, ensuring that only valid coordinates are used for the map. It creates a folium.Map object where the center location is set to the mean latitude and longitude of the filtered 2021 data, and sets an initial zoom level. Within the heatmap, we can set the radius of each heatmap point and the blur level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "884bad2b-f599-4b5c-9d61-9e8c1e6847b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each crime type\n",
    "crime_counts = pandas_df[\"Crm Cd Desc\"].value_counts()\n",
    "\n",
    "# Get the top 10 most frequent crime types\n",
    "top_10_crime_types = crime_counts.head(10)\n",
    "\n",
    "# Display the frequency table\n",
    "print(top_10_crime_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3be41fcf-08d7-4c6c-966d-7eb5275fd395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This code calculates and displays the top 10 most frequent crime types from the \"Crm Cd Desc\" column of the pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d0b0564-8a91-4ce6-bbcf-4c05931ac459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of total crimes for each category\n",
    "total_crimes = len(pandas_df)\n",
    "top_10_crime_percentages = (top_10_crime_types / total_crimes) * 100\n",
    "\n",
    "# Combine the counts and percentages into a DataFrame for better display\n",
    "result_df = pd.DataFrame({\n",
    "    'Count': top_10_crime_types,\n",
    "    'Percentage': top_10_crime_percentages\n",
    "})\n",
    "\n",
    "# Display the frequency table with percentages\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "109c9c97-b55c-47d0-b3d8-738b54a4df20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter data for vehicle stolen crimes\n",
    "pandas_df_vehicle = pandas_df[pandas_df[\"Crm Cd Desc\"] == 'VEHICLE - STOLEN']\n",
    "\n",
    "pandas_df_vehicle[\"LAT\"] = pd.to_numeric(pandas_df_vehicle[\"LAT\"], errors=\"coerce\")\n",
    "pandas_df_vehicle[\"LON\"] = pd.to_numeric(pandas_df_vehicle[\"LON\"], errors=\"coerce\")\n",
    "# Ensure the DataFrame contains valid coordinates\n",
    "pandas_df_vehicle = pandas_df_vehicle.dropna(subset=[\"LAT\", \"LON\"])\n",
    "\n",
    "# Create the map based on vehicle stolen crimes\n",
    "m = folium.Map(location=[pandas_df_vehicle[\"LAT\"].mean(), pandas_df_vehicle[\"LON\"].mean()], zoom_start=10)\n",
    "\n",
    "# Add heatmap\n",
    "HeatMap(pandas_df_vehicle[[\"LAT\", \"LON\"]].values, radius=5, blur=10).add_to(m)\n",
    "\n",
    "# Display map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cebfc08-b166-4119-9af2-af68a6de7bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Repeat the previous folium map code, but this time we filter on observations in the dataset for stolen vehicle crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c55d40-2464-4865-a67e-6c29e916ad1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter data for battery crimes\n",
    "pandas_df_battery = pandas_df[pandas_df[\"Crm Cd Desc\"] == 'BATTERY - SIMPLE ASSAULT']\n",
    "\n",
    "pandas_df_battery[\"LAT\"] = pd.to_numeric(pandas_df_battery[\"LAT\"], errors=\"coerce\")\n",
    "pandas_df_battery[\"LON\"] = pd.to_numeric(pandas_df_battery[\"LON\"], errors=\"coerce\")\n",
    "# Ensure the DataFrame contains valid coordinates\n",
    "pandas_df_battery = pandas_df_battery.dropna(subset=[\"LAT\", \"LON\"])\n",
    "\n",
    "# Create the map based on battery crimes\n",
    "m = folium.Map(location=[pandas_df_battery[\"LAT\"].mean(), pandas_df_battery[\"LON\"].mean()], zoom_start=10)\n",
    "\n",
    "# Add heatmap\n",
    "HeatMap(pandas_df_battery[[\"LAT\", \"LON\"]].values, radius=5, blur=10).add_to(m)\n",
    "\n",
    "# Display map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1987f4b4-310e-4330-ba87-355d1aa2481a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Repeat the previous folium map code, but this time we filter on observations in the dataset for battery crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30c4d41d-6d49-4eb5-82da-df722f8b70c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter data for intimate partner assult\n",
    "pandas_df_partner = pandas_df[pandas_df[\"Crm Cd Desc\"] == 'INTIMATE PARTNER - SIMPLE ASSAULT']\n",
    "\n",
    "pandas_df_partner[\"LAT\"] = pd.to_numeric(pandas_df_partner[\"LAT\"], errors=\"coerce\")\n",
    "pandas_df_partner[\"LON\"] = pd.to_numeric(pandas_df_partner[\"LON\"], errors=\"coerce\")\n",
    "# Ensure the DataFrame contains valid coordinates\n",
    "pandas_df_partner = pandas_df_partner.dropna(subset=[\"LAT\", \"LON\"])\n",
    "\n",
    "# Create the map based on intimate partner assault\n",
    "m = folium.Map(location=[pandas_df_partner[\"LAT\"].mean(), pandas_df_partner[\"LON\"].mean()], zoom_start=10)\n",
    "\n",
    "# Add heatmap\n",
    "HeatMap(pandas_df_partner[[\"LAT\", \"LON\"]].values, radius=5, blur=10).add_to(m)\n",
    "\n",
    "# Display map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8a14973-deb8-4d15-bc07-0cd379528116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Repeat the previous folium code, but this time we filter on observations in the dataset for intimate partner assault crimes."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2088868209064735,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "LA Crime Analysis Notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
